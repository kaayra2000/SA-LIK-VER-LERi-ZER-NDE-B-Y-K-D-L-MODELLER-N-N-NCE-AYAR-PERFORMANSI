# Çeviri veri seti oluşturmuşlar ve GPT4'e de veri ürettirmişler
**MFA Introducing cosmosGPT: Monolingual Training for Turkish Language Models**


CREATING FINETUNE DATASET
To create instruction versions of the models and develop the
best finetuning datasets, we have compiled various datasets
and merged shared collections. Finding a suitable dataset for
finetuning has been one of the focal points of our research.
Original datasets have been created using various sources,
including open-source platforms, contributions from individuals, and data generated by GPT-4 [7]. Initially, four primary
datasets were established. Subsequently, through cleaning and
various combinations, new datasets have been derived. Here
are the names and features of the mentioned primary datasets:


# Çok dilli modeller varken özel modellere neden ihtiyaç var?
**TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation**

However, they often do not perform well in tasks requiring a deep
understanding of language-specific nuances, such
as dependency parsing and named entity recognition (Virtanen et al., 2019; Baumann, 2019; Tanvir
et al., 2021) and lag behind monolingual models
of the same scale (Rust et al., 2021; Nozza et al.,
2020).

# Neden fine tune'e ihtiyaç var?
**Comparison_of_Pretrained_Models_for_Optimized_Transformer_Based_Question_Answering_System**

burada fine tune sonrası başarımdan bahsediliyor

The findings underscore the
influence of model architecture, training strategies, and
dataset characteristics on overall performance. Future
research could explore fine-tuning tailored for health
datasets, advancing question-answering capabilities in the
medical domain. These insights benefit practitioners and
researchers navigating the deployment of transformer-based
models in question-answering systems, emphasizing
considerations for specific domain requirements and
challenges

# Eklenebilecek alt başlıklar
**TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation**

* Conclusion kısmına **limitler**