# DiLoCo: Distributed Low-CommunicationTraining of Language Models.

## Bu makalede kullanılan kaynaklar
Diederik P. Kingma and Jimmy Ba. Adam: A
method for stochastic optimization. Proceedings of the International Conference on Learning
Representations (ICLR), 2014.

Ilya Loshchilov and Frank Hutter. Decoupled
weight decay regularization. Proceedings of the
International Conference on Learning Representations (ICLR), 2019.


# MFA Introducing cosmosGPT: Monolingual Training for Turkish Language Models

* Adam'ı kullanmış

The training of the models was conducted using Google
Cloud’s TPUv3-8 infrastructure. The Large model was trained
with a batch size of 16 and for 2 epochs, while the Medium
model used a batch size of 128 and was trained for 3 epochs.
The training process was optimized using an Adam optimizer
with a learning rate of 1e−4 with linear decay. This approach
has enabled both rapid and efficient training of the models.
In conclusion, this training process and the use of highquality datasets have enabled us to achieve significant advancements in the accuracy and reliability of our Turkish language
models.